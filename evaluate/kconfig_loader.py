"""
Kconfig Loader for FinanceBench Evaluator.

Loads .config files generated by menuconfig and converts them into
Python dataclasses for use by the evaluator.
"""

import os
import sys
from pathlib import Path
from typing import Optional, List, Dict, Any
from dataclasses import dataclass, field


@dataclass
class GlobalConfig:
    """Global execution settings."""
    output_dir: str = "evaluate/results"
    skip_accuracy: bool = False
    use_cache: bool = True


@dataclass
class DatasetConfig:
    """Dataset loading configuration."""
    path: str = ""
    pdf_dir: str = ""  # Directory containing SEC filing PDFs
    filter_numerical: bool = False
    max_samples: Optional[int] = None
    sample_indices: Optional[List[int]] = None


@dataclass
class LocalModelConfig:
    """Local model configuration."""
    name: str = "llama3.2:3b"
    temperature: float = 0.2
    num_ctx: int = 4096
    backend: str = "ollama"  # "ollama" or "sglang"
    sglang_base_url: str = "http://localhost:8000/v1"
    logit_processor_path: Optional[str] = None  # Path to learned logit processor file
    # WFSA length prior parameters (SGLang only)
    generation_strategy: str = "sequential"  # "sequential" or "single_shot"
    beta_explanation: float = 1.0  # WFSA strength for explanation field
    beta_citation: float = 2.0     # WFSA strength for citation field
    beta_answer: float = 1.5       # WFSA strength for answer field
    min_tokens_explanation: int = 10  # Minimum tokens for explanation
    min_tokens_citation: int = 5      # Minimum tokens for citation
    min_tokens_answer: int = 3        # Minimum tokens for answer
    max_tokens_explanation: int = 200  # Maximum tokens for explanation
    max_tokens_citation: int = 150     # Maximum tokens for citation
    max_tokens_answer: int = 100       # Maximum tokens for answer


@dataclass
class RemoteModelConfig:
    """Remote model configuration."""
    name: str = "gpt-4o"
    temperature: float = 0.0


@dataclass
class ModelsConfig:
    """Local and remote model configurations."""
    local: LocalModelConfig = field(default_factory=LocalModelConfig)
    remote: RemoteModelConfig = field(default_factory=RemoteModelConfig)


@dataclass
class CommonProtocolConfig:
    """Settings common to multiple protocols."""
    max_rounds: int = 2
    num_samples_per_task: int = 1


@dataclass
class MinionsProtocolConfig:
    """MINIONS protocol configuration."""
    num_tasks_per_round: int = 3
    chunk_fn: str = "chunk_by_section"
    max_chunk_size: int = 3000
    pages_per_chunk: int = 5
    max_jobs_per_round: Optional[int] = None
    use_retrieval: Optional[str] = None
    retrieval_model: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dict, excluding None values."""
        return {k: v for k, v in {
            'num_tasks_per_round': self.num_tasks_per_round,
            'chunk_fn': self.chunk_fn,
            'max_chunk_size': self.max_chunk_size,
            'pages_per_chunk': self.pages_per_chunk,
            'max_jobs_per_round': self.max_jobs_per_round,
            'use_retrieval': self.use_retrieval,
            'retrieval_model': self.retrieval_model,
        }.items() if v is not None}


@dataclass
class ProtocolsConfig:
    """Protocol configuration and selection."""
    active: List[str] = field(default_factory=lambda: ['minions'])
    common: CommonProtocolConfig = field(default_factory=CommonProtocolConfig)
    minions: MinionsProtocolConfig = field(default_factory=MinionsProtocolConfig)


@dataclass
class EvaluatorConfig:
    """Complete evaluator configuration."""
    dataset: DatasetConfig = field(default_factory=DatasetConfig)
    models: ModelsConfig = field(default_factory=ModelsConfig)
    protocols: ProtocolsConfig = field(default_factory=ProtocolsConfig)
    global_config: GlobalConfig = field(default_factory=GlobalConfig)

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return {
            'global': {
                'output_dir': self.global_config.output_dir,
                'skip_accuracy': self.global_config.skip_accuracy,
                'use_cache': self.global_config.use_cache,
            },
            'dataset': {
                'path': self.dataset.path,
                'pdf_dir': self.dataset.pdf_dir,
                'filter_numerical': self.dataset.filter_numerical,
                'max_samples': self.dataset.max_samples,
                'sample_indices': self.dataset.sample_indices,
            },
            'models': {
                'local': {
                    'name': self.models.local.name,
                    'temperature': self.models.local.temperature,
                    'num_ctx': self.models.local.num_ctx,
                    'backend': self.models.local.backend,
                    'sglang_base_url': self.models.local.sglang_base_url,
                    'logit_processor_path': self.models.local.logit_processor_path,
                    'generation_strategy': self.models.local.generation_strategy,
                    'beta_explanation': self.models.local.beta_explanation,
                    'beta_citation': self.models.local.beta_citation,
                    'beta_answer': self.models.local.beta_answer,
                    'min_tokens_explanation': self.models.local.min_tokens_explanation,
                    'min_tokens_citation': self.models.local.min_tokens_citation,
                    'min_tokens_answer': self.models.local.min_tokens_answer,
                    'max_tokens_explanation': self.models.local.max_tokens_explanation,
                    'max_tokens_citation': self.models.local.max_tokens_citation,
                    'max_tokens_answer': self.models.local.max_tokens_answer,
                },
                'remote': {
                    'name': self.models.remote.name,
                    'temperature': self.models.remote.temperature,
                },
            },
            'protocols': {
                'active': self.protocols.active,
                'common': {
                    'max_rounds': self.protocols.common.max_rounds,
                    'num_samples_per_task': self.protocols.common.num_samples_per_task,
                },
                'minions': self.protocols.minions.to_dict(),
            }
        }

    def validate(self) -> List[str]:
        """Custom validation."""
        errors = []
        
        if not self.dataset.path:
            errors.append("Dataset path is required")
        elif not Path(self.dataset.path).exists():
            errors.append(f"Dataset path does not exist: {self.dataset.path}")
        
        if not self.dataset.pdf_dir:
            errors.append("PDF directory is required")
        elif not Path(self.dataset.pdf_dir).exists():
            errors.append(f"PDF directory does not exist: {self.dataset.pdf_dir}")
        
        if not self.protocols.active:
            errors.append("At least one protocol must be selected")
        
        # Validate active protocols are supported
        supported = {'minions', 'minion', 'remote_only', 'local_only'}
        for protocol in self.protocols.active:
            if protocol not in supported:
                errors.append(f"Unsupported protocol: {protocol}")
        
        return errors


class KconfigLoader:
    """Loads .config files and converts to EvaluatorConfig."""
    
    @staticmethod
    def load_config(config_path: str) -> EvaluatorConfig:
        """
        Load configuration from a .config file.
        
        Args:
            config_path: Path to .config file
            
        Returns:
            EvaluatorConfig object
        """
        config_path = Path(config_path)
        if not config_path.exists():
            raise FileNotFoundError(f"Config file not found: {config_path}")
        
        # Parse the .config file
        values = KconfigLoader._parse_config_file(config_path)
        
        # Build configuration object
        config = KconfigLoader._build_config(values)
        
        # Validate
        errors = config.validate()
        if errors:
            raise ValueError("Configuration validation failed:\n" + "\n".join(f"  - {e}" for e in errors))
        
        return config
    
    @staticmethod
    def _parse_config_file(config_path: Path) -> Dict[str, str]:
        """Parse a .config file into key-value pairs."""
        values = {}
        
        with open(config_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                
                # Skip comments and empty lines
                if not line or line.startswith('#'):
                    continue
                
                # Parse CONFIG_KEY=value
                if '=' in line:
                    key, value = line.split('=', 1)
                    key = key.strip()
                    value = value.strip()
                    
                    # Remove CONFIG_ prefix
                    if key.startswith('CONFIG_'):
                        key = key[7:]
                    
                    # Remove quotes from strings
                    if value.startswith('"') and value.endswith('"'):
                        value = value[1:-1]
                    
                    values[key] = value
        
        return values
    
    @staticmethod
    def _build_config(values: Dict[str, str]) -> EvaluatorConfig:
        """Build EvaluatorConfig from parsed values."""
        config = EvaluatorConfig()
        
        # Dataset
        config.dataset.path = values.get('DATASET_PATH', '')
        config.dataset.pdf_dir = values.get('DATASET_PDF_DIR', '')
        # Note: When a bool is "not set" in Kconfig (commented out), default to 'n' (disabled)
        config.dataset.filter_numerical = values.get('FILTER_NUMERICAL', 'n') == 'y'
        
        max_samples = int(values.get('MAX_SAMPLES', '0'))
        config.dataset.max_samples = max_samples if max_samples > 0 else None
        
        if values.get('USE_SAMPLE_INDICES', 'n') == 'y':
            indices_str = values.get('SAMPLE_INDICES', '')
            if indices_str:
                try:
                    config.dataset.sample_indices = [int(x.strip()) for x in indices_str.split(',')]
                except ValueError:
                    pass
        elif values.get('USE_SAMPLE_RANGE', 'n') == 'y':
            range_str = values.get('SAMPLE_RANGE', '')
            if range_str and '-' in range_str:
                try:
                    start, end = range_str.split('-', 1)
                    start, end = int(start.strip()), int(end.strip())
                    config.dataset.sample_indices = list(range(start, end + 1))
                except ValueError:
                    pass
        
        # Models
        config.models.local.name = values.get('LOCAL_MODEL_NAME', 'llama3.2:3b')
        config.models.local.temperature = int(values.get('LOCAL_TEMPERATURE', '20')) / 100.0
        config.models.local.num_ctx = int(values.get('LOCAL_NUM_CTX', '4096'))
        
        # Local backend: sglang or ollama
        if values.get('LOCAL_BACKEND_SGLANG', 'n') == 'y':
            config.models.local.backend = 'sglang'
            config.models.local.sglang_base_url = values.get('SGLANG_BASE_URL', 'http://localhost:8000/v1')
            # Logit processor path (optional, for constraint decoding)
            logit_processor_path = values.get('LOGIT_PROCESSOR_PATH', '')
            config.models.local.logit_processor_path = logit_processor_path if logit_processor_path else None
        else:
            config.models.local.backend = 'ollama'
        
        # Generation strategy (SGLang only)
        if values.get('GENERATION_SINGLE_SHOT', 'n') == 'y':
            config.models.local.generation_strategy = 'single_shot'
        else:
            config.models.local.generation_strategy = 'sequential'
        
        # WFSA length prior parameters (beta values stored as x100 integers in Kconfig)
        config.models.local.beta_explanation = int(values.get('WFSA_BETA_EXPLANATION', '100')) / 100.0
        config.models.local.beta_citation = int(values.get('WFSA_BETA_CITATION', '200')) / 100.0
        config.models.local.beta_answer = int(values.get('WFSA_BETA_ANSWER', '150')) / 100.0
        config.models.local.min_tokens_explanation = int(values.get('WFSA_MIN_TOKENS_EXPLANATION', '10'))
        config.models.local.min_tokens_citation = int(values.get('WFSA_MIN_TOKENS_CITATION', '5'))
        config.models.local.min_tokens_answer = int(values.get('WFSA_MIN_TOKENS_ANSWER', '3'))
        config.models.local.max_tokens_explanation = int(values.get('WFSA_MAX_TOKENS_EXPLANATION', '200'))
        config.models.local.max_tokens_citation = int(values.get('WFSA_MAX_TOKENS_CITATION', '150'))
        config.models.local.max_tokens_answer = int(values.get('WFSA_MAX_TOKENS_ANSWER', '100'))
        
        config.models.remote.name = values.get('REMOTE_MODEL_NAME', 'gpt-4o')
        config.models.remote.temperature = int(values.get('REMOTE_TEMPERATURE', '0')) / 100.0
        
        # Protocols - active list
        active_protocols = []
        if values.get('PROTOCOL_MINIONS', 'n') == 'y':
            active_protocols.append('minions')
        if values.get('PROTOCOL_MINION', 'n') == 'y':
            active_protocols.append('minion')
        if values.get('PROTOCOL_REMOTE_ONLY', 'n') == 'y':
            active_protocols.append('remote_only')
        if values.get('PROTOCOL_LOCAL_ONLY', 'n') == 'y':
            active_protocols.append('local_only')
        
        config.protocols.active = active_protocols if active_protocols else ['minions']
        
        # Common protocol settings
        config.protocols.common.max_rounds = int(values.get('MAX_ROUNDS', '2'))
        config.protocols.common.num_samples_per_task = int(values.get('NUM_SAMPLES_PER_TASK', '1'))
        
        # MINIONS settings
        config.protocols.minions.num_tasks_per_round = int(values.get('MINIONS_TASKS_PER_ROUND', '3'))
        config.protocols.minions.max_chunk_size = int(values.get('MINIONS_MAX_CHUNK_SIZE', '3000'))
        
        max_jobs = int(values.get('MINIONS_MAX_JOBS_PER_ROUND', '0'))
        config.protocols.minions.max_jobs_per_round = max_jobs if max_jobs > 0 else None
        
        # Chunk function
        if values.get('MINIONS_CHUNK_BY_SECTION', 'y') == 'y':
            config.protocols.minions.chunk_fn = 'chunk_by_section'
        elif values.get('MINIONS_CHUNK_BY_PAGE', 'n') == 'y':
            config.protocols.minions.chunk_fn = 'chunk_by_page'
        elif values.get('MINIONS_CHUNK_MULTIPLE_PAGES', 'n') == 'y':
            config.protocols.minions.chunk_fn = 'chunk_on_multiple_pages'
            config.protocols.minions.pages_per_chunk = int(values.get('MINIONS_PAGES_PER_CHUNK', '5'))
        elif values.get('MINIONS_CHUNK_BY_PARAGRAPH', 'n') == 'y':
            config.protocols.minions.chunk_fn = 'chunk_by_paragraph'
        elif values.get('MINIONS_CHUNK_HIERARCHICAL', 'n') == 'y':
            config.protocols.minions.chunk_fn = 'hierarchical'
        
        # Retrieval
        if values.get('MINIONS_RETRIEVAL_BM25', 'n') == 'y':
            config.protocols.minions.use_retrieval = 'bm25'
        elif values.get('MINIONS_RETRIEVAL_EMBEDDING', 'n') == 'y':
            config.protocols.minions.use_retrieval = 'embedding'
            config.protocols.minions.retrieval_model = values.get('MINIONS_RETRIEVAL_MODEL', '')
        
        # Global settings
        config.global_config.output_dir = values.get('OUTPUT_DIR', 'evaluate/results')
        config.global_config.skip_accuracy = values.get('SKIP_ACCURACY', 'n') == 'y'
        config.global_config.use_cache = values.get('USE_CACHE', 'y') == 'y'
        
        return config


def load_config(config_path: str) -> EvaluatorConfig:
    """
    Convenience function to load configuration.
    
    Args:
        config_path: Path to .config file
        
    Returns:
        Validated EvaluatorConfig object
    """
    return KconfigLoader.load_config(config_path)
